There is an error-free tree for any training set, which leads to the problem of overfitting. 
Find the right stopping criterion to solve the problem is challenging. 
One solution is pruning after the whole tree is constructed, we can cut some nodes. 
Such an operation can be performed using a test or validation set. 
Pruning can reduce the complexity of the final classifier, and improve perdictive accuracy by reducing overfitting. 

	The pruning algorithm is form as follows: 

	1) We build a tree for the training set. 

	2) Then, we pass a validation set through the constructed tree, and consider any internal node t and its left and right sub-nodes L(sub(t)) and R(sub(t)). 

	3) If no one object from the validation sample has reached t, then we can say that this node is insignificant, and make t the leaf

	4) If objects from the validation set have reached t, then we have to consider the following three values: 
		
		A) The number of classification error from a subtree of t

		B) The number of classification error from the L(sub(t)) subtree

	
		C) The number of classification error from the R(sub(t)) subtree



If the value of th efirst case is zero, then we make node t as a leaf node with the corresponding predicition for the class. 
Overwise we choose the minimum of these values. 
Depending on which of them is minimal, we do the following repectively: 

	1) if the firs is minimal, do nothing 

	2) if the second is minimal, replace the tree from node t with a subtree from node L(sub(t)) 

	3) If the third is minimal, replace the tree from node t with a subtree from node R(sub(t))


Such a procedure regularizes the algorithm to beat overfitting and increase the ability to generalize. 
In the case of a k-dimensional tree, different approaches can be used to select the forecast in the leaf. 
We can take the most common class among the object of the training that fall in this leaf for classification. 
Alternatlively, we can calculate the average of the objective function of these objects for regression. 


