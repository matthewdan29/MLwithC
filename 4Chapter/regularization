Regularization is a technique that's used to reduce model overfitting. 
There are two main approaches to regularization. 	
		1) Training data preprocessing. 
		
		2) Loss Function Modification. 

The main idea of the loss function modification techniques is to add terms to the loss function that penalize algorithm results, therby leading to significant varicnace. 
The idea of training data preprocessing techniques is to add more distinct training samples. 
Usually, in such an approach, new training samples are generated by augmenting exiting ones. 
In general, both approaches add some prior knowledge about the task domain to the model. 
This additional information helps us with variance regulaization. 
Therefor we can conclude that regularization is any technique that leads to minimizing the generalization error. 
